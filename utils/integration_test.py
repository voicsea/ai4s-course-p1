#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é›†æˆæµ‹è¯•è„šæœ¬ - éªŒè¯æ‰€æœ‰æ–°æ¨¡å—çš„æ­£ç¡®é›†æˆ
Integration Test Script - Validates all new modules work correctly

æµ‹è¯•å†…å®¹:
1. é…ç½®ç³»ç»Ÿ (Config System)
2. æ¨¡å‹æ³¨å†Œè¡¨ (Model Registry)
3. è¯„ä¼°æŒ‡æ ‡ (Metrics)
4. å¯è§†åŒ–å·¥å…· (Visualization)
5. å®Œæ•´å·¥ä½œæµ (Complete Workflow)
"""

import sys
import json
import torch
import numpy as np
from pathlib import Path

# è®¾ç½®æ ‡å‡†è¾“å‡ºç¼–ç 
import io
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

# æ·»åŠ é¡¹ç›®è·¯å¾„
PROJECT_ROOT = Path(__file__).parent
sys.path.insert(0, str(PROJECT_ROOT))


def test_imports():
    """æµ‹è¯•1: æ‰€æœ‰æ¨¡å—å¯ä»¥æˆåŠŸå¯¼å…¥"""
    print("\n" + "="*60)
    print("æµ‹è¯•1: æ¨¡å—å¯¼å…¥")
    print("="*60)
    
    try:
        from configs.config import TrainConfig, EvalConfig
        print("âœ“ configs.config å¯¼å…¥æˆåŠŸ")
    except Exception as e:
        print(f"âœ— configs.config å¯¼å…¥å¤±è´¥: {e}")
        return False
    
    try:
        from models.registry import ModelRegistry, ModelFactory
        print("âœ“ models.registry å¯¼å…¥æˆåŠŸ")
    except Exception as e:
        print(f"âœ— models.registry å¯¼å…¥å¤±è´¥: {e}")
        return False
    
    try:
        from evaluation.metrics import Evaluator
        print("âœ“ evaluation.metrics å¯¼å…¥æˆåŠŸ")
    except Exception as e:
        print(f"âœ— evaluation.metrics å¯¼å…¥å¤±è´¥: {e}")
        return False
    
    try:
        from visualization.visualize import Visualizer
        print("âœ“ visualization.visualize å¯¼å…¥æˆåŠŸ")
    except Exception as e:
        print(f"âœ— visualization.visualize å¯¼å…¥å¤±è´¥: {e}")
        return False
    
    try:
        from train import AdvancedTrainer
        print("âœ“ train.AdvancedTrainer å¯¼å…¥æˆåŠŸ")
    except Exception as e:
        print(f"âš  train.AdvancedTrainer å¯¼å…¥å¤±è´¥ (éå…³é”®): {e}")
        # ä¸æ˜¯å…³é”®æ¨¡å—ï¼Œç»§ç»­æµ‹è¯•
    
    try:
        # test.py now embeds the comprehensive evaluator and SOTA functionality
        from test import ComprehensiveEvaluator, SOTAEvaluator
        print("âœ“ test.ComprehensiveEvaluator / SOTAEvaluator å¯¼å…¥æˆåŠŸ")
    except Exception as e:
        print(f"âœ— test.ComprehensiveEvaluator å¯¼å…¥å¤±è´¥: {e}")
        return False
    
    return True


def test_config_system():
    """æµ‹è¯•2: é…ç½®ç®¡ç†ç³»ç»Ÿ"""
    print("\n" + "="*60)
    print("æµ‹è¯•2: é…ç½®ç®¡ç†ç³»ç»Ÿ")
    print("="*60)
    
    from configs.config import TrainConfig, EvalConfig
    
    # æµ‹è¯•åˆ›å»ºé…ç½®
    try:
        config = TrainConfig(
            model_name='resnet_unet',
            num_epochs=50,
            batch_size=16,
            learning_rate=1e-4
        )
        print(f"âœ“ é…ç½®åˆ›å»ºæˆåŠŸ")
        print(f"  - æ¨¡å‹: {config.model_name}")
        print(f"  - è½®æ¬¡: {config.num_epochs}")
        print(f"  - å­¦ä¹ ç‡: {config.learning_rate}")
    except Exception as e:
        print(f"âœ— é…ç½®åˆ›å»ºå¤±è´¥: {e}")
        return False
    
    # æµ‹è¯•JSONä¿å­˜å’ŒåŠ è½½
    try:
        config_path = PROJECT_ROOT / 'configs' / 'default_config.json'
        if config_path.exists():
            config_loaded = TrainConfig.from_json(str(config_path))
            print(f"âœ“ é…ç½®åŠ è½½æˆåŠŸ (ä»JSON)")
            print(f"  - åŠ è½½çš„æ¨¡å‹: {config_loaded.model_name}")
    except Exception as e:
        print(f"âœ— é…ç½®åŠ è½½å¤±è´¥: {e}")
        return False
    
    return True


def test_model_registry():
    """æµ‹è¯•3: æ¨¡å‹æ³¨å†Œè¡¨"""
    print("\n" + "="*60)
    print("æµ‹è¯•3: æ¨¡å‹æ³¨å†Œè¡¨")
    print("="*60)
    
    from models.registry import ModelRegistry, ModelFactory
    
    try:
        # åˆ—å‡ºæ‰€æœ‰æ¨¡å‹
        models = ModelRegistry.list_models()
        print(f"âœ“ å¯ç”¨æ¨¡å‹: {models}")
        
        if 'resnet_unet' not in models:
            print("âœ— resnet_unet æœªæ³¨å†Œ")
            return False
        
        print("âœ“ resnet_unet å·²æ³¨å†Œ")
    except Exception as e:
        print(f"âœ— æ¨¡å‹åˆ—è¡¨è·å–å¤±è´¥: {e}")
        return False
    
    # æµ‹è¯•æ¨¡å‹åˆ›å»º
    try:
        model = ModelFactory.create('resnet_unet', out_channels=1)
        print(f"âœ“ æ¨¡å‹åˆ›å»ºæˆåŠŸ: resnet_unet")
        
        # è®¡ç®—å‚æ•°é‡
        num_params = sum(p.numel() for p in model.parameters())
        print(f"  - å‚æ•°é‡: {num_params:,}")
    except Exception as e:
        print(f"âœ— æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
        return False
    
    # æµ‹è¯•å¤§æ¨¡å‹
    try:
        model_large = ModelFactory.create('resnet_unet_large', out_channels=1)
        print(f"âœ“ å¤§æ¨¡å‹åˆ›å»ºæˆåŠŸ: resnet_unet_large")
        
        num_params_large = sum(p.numel() for p in model_large.parameters())
        print(f"  - å‚æ•°é‡: {num_params_large:,}")
    except Exception as e:
        print(f"âœ— å¤§æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
        return False
    
    # æµ‹è¯•æ‰©æ•£æ¨¡å‹
    try:
        model_diffusion = ModelFactory.create('diffusion_dem', out_channels=1)
        print(f"âœ“ æ‰©æ•£æ¨¡å‹åˆ›å»ºæˆåŠŸ: diffusion_dem")
        
        num_params_diff = sum(p.numel() for p in model_diffusion.parameters())
        print(f"  - å‚æ•°é‡: {num_params_diff:,}")
    except Exception as e:
        print(f"âœ— æ‰©æ•£æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
        return False
    
    return True


def test_metrics():
    """æµ‹è¯•4: è¯„ä¼°æŒ‡æ ‡"""
    print("\n" + "="*60)
    print("æµ‹è¯•4: è¯„ä¼°æŒ‡æ ‡")
    print("="*60)
    
    from evaluation.metrics import Evaluator
    
    try:
        evaluator = Evaluator()
        print("âœ“ è¯„ä¼°å™¨åˆ›å»ºæˆåŠŸ")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        predictions = torch.randn(8, 1, 128, 128)  # (B, C, H, W)
        targets = torch.randn(8, 1, 128, 128)
        
        # è®¡ç®—æŒ‡æ ‡
        metrics = evaluator.evaluate_batch(predictions, targets)
        
        print("âœ“ æŒ‡æ ‡è®¡ç®—æˆåŠŸ:")
        for metric_name, metric_value in metrics.items():
            print(f"  - {metric_name}: {metric_value:.4f}")
        
        # éªŒè¯æ‰€æœ‰æŒ‡æ ‡éƒ½å­˜åœ¨
        required_metrics = ['mae', 'mse', 'rmse', 'ssim', 'psnr', 'r2']
        for metric in required_metrics:
            if metric not in metrics:
                print(f"âœ— ç¼ºå°‘æŒ‡æ ‡: {metric}")
                return False
        
        print(f"âœ“ æ‰€æœ‰æŒ‡æ ‡ ({len(required_metrics)}) éƒ½å·²è®¡ç®—")
    except Exception as e:
        print(f"âœ— æŒ‡æ ‡è®¡ç®—å¤±è´¥: {e}")
        return False
    
    return True


def test_visualization():
    """æµ‹è¯•5: å¯è§†åŒ–å·¥å…·"""
    print("\n" + "="*60)
    print("æµ‹è¯•5: å¯è§†åŒ–å·¥å…·")
    print("="*60)
    
    try:
        from visualization.visualize import Visualizer
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = PROJECT_ROOT / 'test_visualizations'
        output_dir.mkdir(exist_ok=True)
        
        visualizer = Visualizer(output_dir=str(output_dir))
        print("âœ“ å¯è§†åŒ–å·¥å…·åˆ›å»ºæˆåŠŸ")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ® (åªç”¨å•é€šé“å›¾åƒé¿å…RGBé—®é¢˜)
        images = torch.randn(4, 1, 128, 128)
        predictions = torch.randn(4, 1, 128, 128).abs()
        targets = torch.randn(4, 1, 128, 128).abs()
        
        # æµ‹è¯•é¢„æµ‹å¯è§†åŒ–
        try:
            visualizer.visualize_predictions(images, predictions, targets)
            print("âœ“ é¢„æµ‹å¯è§†åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"âš  é¢„æµ‹å¯è§†åŒ–å¤±è´¥ (éå…³é”®): {e}")
        
        # æµ‹è¯•è¯¯å·®åˆ†å¸ƒ
        try:
            visualizer.plot_error_map(predictions, targets)
            print("âœ“ è¯¯å·®åˆ†å¸ƒå¯è§†åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"âš  è¯¯å·®åˆ†å¸ƒå¯è§†åŒ–å¤±è´¥: {e}")
        
        # æµ‹è¯•æ•£ç‚¹åˆ†æ
        try:
            visualizer.plot_scatter_analysis(predictions, targets)
            print("âœ“ æ•£ç‚¹åˆ†æå¯è§†åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"âš  æ•£ç‚¹åˆ†æå¯è§†åŒ–å¤±è´¥: {e}")
        
        # æ¸…ç†æµ‹è¯•è¾“å‡º
        import shutil
        shutil.rmtree(output_dir, ignore_errors=True)
        
    except Exception as e:
        print(f"âœ— å¯è§†åŒ–å·¥å…·æµ‹è¯•å¤±è´¥: {e}")
        return False
    
    return True


def test_complete_workflow():
    """æµ‹è¯•6: å®Œæ•´å·¥ä½œæµ"""
    print("\n" + "="*60)
    print("æµ‹è¯•6: å®Œæ•´å·¥ä½œæµé›†æˆ")
    print("="*60)
    
    try:
        from configs.config import TrainConfig, EvalConfig
        from models.registry import ModelRegistry, ModelFactory
        from evaluation.metrics import Evaluator
        
        # æ­¥éª¤1: åˆ›å»ºé…ç½®
        print("\n[æ­¥éª¤1] åˆ›å»ºé…ç½®...")
        config = TrainConfig(
            model_name='resnet_unet',
            num_epochs=1,
            batch_size=4,
            learning_rate=1e-4
        )
        print("âœ“ é…ç½®åˆ›å»ºå®Œæˆ")
        
        # æ­¥éª¤2: åˆ›å»ºæ¨¡å‹
        print("\n[æ­¥éª¤2] åˆ›å»ºæ¨¡å‹...")
        model = ModelFactory.create(config.model_name, out_channels=1)
        print(f"âœ“ æ¨¡å‹ {config.model_name} åˆ›å»ºå®Œæˆ")
        
        # æ­¥éª¤3: æ¨¡å‹æ¨ç†
        print("\n[æ­¥éª¤3] æ¨¡å‹æ¨ç†...")
        model.eval()
        with torch.no_grad():
            sample_input = torch.randn(2, 3, 256, 256)
            sample_output = model(sample_input)
        print(f"âœ“ æ¨ç†æˆåŠŸ (è¾“å‡ºå½¢çŠ¶: {sample_output.shape})")
        
        # æ­¥éª¤4: è¯„ä¼°æŒ‡æ ‡
        print("\n[æ­¥éª¤4] è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
        evaluator = Evaluator()
        test_pred = torch.randn(4, 1, 128, 128).abs()
        test_target = torch.randn(4, 1, 128, 128).abs()
        metrics = evaluator.evaluate_batch(test_pred, test_target)
        print("âœ“ æŒ‡æ ‡è®¡ç®—å®Œæˆ:")
        for name, value in list(metrics.items())[:3]:  # æ˜¾ç¤ºå‰3ä¸ª
            print(f"  - {name}: {value:.4f}")
        
        # æ­¥éª¤5: å¤šæ¨¡å‹å¯¹æ¯”
        print("\n[æ­¥éª¤5] å¤šæ¨¡å‹å¯¹æ¯”...")
        all_models = ModelRegistry.list_models()
        
        results = {}
        for model_name in all_models:
            m = ModelFactory.create(model_name, out_channels=1)
            params = sum(p.numel() for p in m.parameters())
            results[model_name] = {
                'parameters': params,
                'status': 'ready'
            }
        
        print(f"âœ“ æ¨¡å‹å¯¹æ¯”å®Œæˆ ({len(results)} ä¸ªæ¨¡å‹)")
        for model_name, info in results.items():
            print(f"  - {model_name}: {info['parameters']:,} å‚æ•°")
        
        print("\n" + "="*60)
        print("âœ“ å®Œæ•´å·¥ä½œæµæµ‹è¯•PASSED")
        print("="*60)
        
    except Exception as e:
        print(f"\nâœ— å®Œæ•´å·¥ä½œæµæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    return True


def test_ddpm_and_amp():
    """æµ‹è¯•7: DDPM æŸå¤±å‡½æ•°å’Œ AMP æ··åˆç²¾åº¦æ”¯æŒ"""
    print("\n" + "="*60)
    print("æµ‹è¯•7: DDPM æŸå¤±å‡½æ•°å’Œ AMP æ”¯æŒ")
    print("="*60)
    
    try:
        from losses import DDPMNoiseLoss
        from torch.cuda.amp import autocast, GradScaler
        
        # æµ‹è¯• DDPM æŸå¤±
        batch_size, channels, H, W = 4, 1, 128, 128
        noise_pred = torch.randn(batch_size, channels, H, W)
        noise_target = torch.randn(batch_size, channels, H, W)
        
        ddpm_loss = DDPMNoiseLoss(loss_type='l2')
        loss = ddpm_loss(noise_pred, noise_target)
        print(f"âœ“ DDPM L2 æŸå¤±: {loss.item():.6f}")
        
        # æµ‹è¯• AMP (å¦‚æœ CUDA å¯ç”¨)
        if torch.cuda.is_available():
            device = torch.device('cuda')
            model = torch.nn.Linear(100, 50).to(device)
            input_data = torch.randn(32, 100, device=device)
            target = torch.randn(32, 50, device=device)
            
            optimizer = torch.optim.Adam(model.parameters())
            scaler = GradScaler()
            criterion = torch.nn.MSELoss()
            
            with autocast():
                output = model(input_data)
                loss = criterion(output, target)
            
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            print(f"âœ“ AMP æ··åˆç²¾åº¦è®­ç»ƒæˆåŠŸï¼ŒæŸå¤±: {loss.item():.6f}")
        else:
            print("âŠ˜ CUDA ä¸å¯ç”¨ï¼Œè·³è¿‡ AMP æµ‹è¯•")
        
        return True
    except Exception as e:
        print(f"âœ— DDPM å’Œ AMP æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_diffusion_model():
    """æµ‹è¯•8: Diffusion DEM æ¨¡å‹"""
    print("\n" + "="*60)
    print("æµ‹è¯•8: Diffusion DEM æ¨¡å‹")
    print("="*60)
    
    try:
        from models.diffusion import DiffusionDEM
        
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        model = DiffusionDEM(out_channels=1, num_timesteps=1000).to(device)
        print(f"âœ“ DiffusionDEM æ¨¡å‹åˆ›å»ºæˆåŠŸ")
        
        # å‚æ•°ç»Ÿè®¡
        total_params = sum(p.numel() for p in model.parameters())
        print(f"âœ“ æ¨¡å‹å‚æ•°æ•°: {total_params:,}")
        
        # æµ‹è¯•å‰å‘æ‰©æ•£
        rgb = torch.randn(2, 3, 256, 256).to(device)
        dem = torch.randn(2, 1, 256, 256).to(device)
        t = torch.randint(0, 1000, (2,), device=device, dtype=torch.long)
        
        x_t, eps_target = model.ddpm_forward(dem, t)
        print(f"âœ“ DDPM å‰å‘æ‰©æ•£æˆåŠŸ, x_t: {x_t.shape}, eps_target: {eps_target.shape}")
        
        # æµ‹è¯•å»å™ª
        with torch.no_grad():
            eps_pred = model._denoise_step(x_t, t, condition_rgb=rgb)
        print(f"âœ“ å»å™ªæ­¥éª¤æˆåŠŸ, è¾“å‡º: {eps_pred.shape}")
        
        # æµ‹è¯•æ¨ç†
        with torch.no_grad():
            dem_gen = model.inference(rgb, num_steps=5)
        print(f"âœ“ æ¨ç†æˆåŠŸ, ç”Ÿæˆçš„ DEM: {dem_gen.shape}")
        
        return True
    except Exception as e:
        print(f"âœ— Diffusion æ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("\n" + "â–ˆ"*60)
    print("â–ˆ" + " "*58 + "â–ˆ")
    print("â–ˆ" + "  UNet é¡¹ç›®ç»¼åˆé›†æˆæµ‹è¯•".center(58) + "â–ˆ")
    print("â–ˆ" + " "*58 + "â–ˆ")
    print("â–ˆ"*60)
    
    tests = [
        ("æ¨¡å—å¯¼å…¥", test_imports),
        ("é…ç½®ç³»ç»Ÿ", test_config_system),
        ("æ¨¡å‹æ³¨å†Œè¡¨", test_model_registry),
        ("è¯„ä¼°æŒ‡æ ‡", test_metrics),
        ("å¯è§†åŒ–å·¥å…·", test_visualization),
        ("å®Œæ•´å·¥ä½œæµ", test_complete_workflow),
        ("DDPM å’Œ AMP", test_ddpm_and_amp),
        ("Diffusion æ¨¡å‹", test_diffusion_model),
    ]
    
    results = {}
    for test_name, test_func in tests:
        try:
            results[test_name] = test_func()
        except Exception as e:
            print(f"\nâœ— {test_name} å¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
            results[test_name] = False
    
    # æ‰“å°æ€»ç»“
    print("\n" + "â–ˆ"*60)
    print("â–ˆ" + "æµ‹è¯•æ€»ç»“".center(58) + "â–ˆ")
    print("â–ˆ"*60)
    
    passed = sum(1 for v in results.values() if v)
    total = len(results)
    
    for test_name, result in results.items():
        status = "âœ“ PASS" if result else "âœ— FAIL"
        print(f"{status:7} | {test_name}")
    
    print("â–ˆ"*60)
    print(f"æ€»è®¡: {passed}/{total} ä¸ªæµ‹è¯•é€šè¿‡")
    
    if passed == total:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•éƒ½é€šè¿‡äº†ï¼é¡¹ç›®é‡æ„å®Œæˆå¹¶å¯ç”¨ã€‚")
        return 0
    else:
        print(f"\nâš  æœ‰ {total - passed} ä¸ªæµ‹è¯•å¤±è´¥")
        return 1


if __name__ == '__main__':
    sys.exit(main())
